% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.1 distribution.
%   Version 4.1r of REVTeX, August 2010
%
%   Copyright (c) 2009, 2010 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[
reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
showpacs,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,
amssymb,
aps,
pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
longbibliography
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{color}% Allows color text
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\usepackage{ulem}% allows strike-out using sout
\usepackage[caption=false]{subfig}

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\providecommand{\auedit}[1]{#1}
\newif\ifcmnt
%  Use \cmntfalse to not see comments when it is latex'ed
%\cmntfalse
%  Use \cmnttrue to see the comments
\cmnttrue

\ifcmnt
    \providecommand{\aucmnt}[1]{#1}
\else
    \providecommand{\aucmnt}[1]{}
\fi
\newcommand{\HV}[1]{\auedit{\textcolor{blue}{#1}}}
\newcommand{\HVc}[1]{\aucmnt{\textcolor{blue}{[HV: #1]}}}
\newcommand{\HVs}[1]{\auedit{\textcolor{blue}{\sout{#1}}}}
\newcommand{\SG}[1]{\auedit{\textcolor{magenta}{#1}}}
\newcommand{\SGc}[1]{\aucmnt{\textcolor{magenta}{[SG: #1]}}}
\newcommand{\SGs}[1]{\auedit{\textcolor{magenta}{\sout{#1}}}}
\newcommand{\LS}[1]{\auedit{\textcolor{green}{#1}}}
\newcommand{\LSc}[1]{\aucmnt{\textcolor{green}{[GS: #1]}}}
\newcommand{\TC}[1]{\auedit{\textcolor{red}{#1}}}

\newcommand{\rhotrue}{\rho_{\text{true}}}

\begin{document}

% \preprint{APS/123-QED}

\title{Quadrature Histograms in Maximum Likelihood Quantum State Tomography}% Force line breaks with \\
%\thanks{A footnote to the article title}%
\author{J. L. E. Silva}
\affiliation{Departamento de Engenharia de Teleinform\'atica, Universidade Federal do Cear\'a, Fortaleza, Cear\'a, 60440, Brazil}
\author{H. M. Vasconcelos}
\email{hilma@ufc.br}
\affiliation{Departamento de Engenharia de Teleinform\'atica, Universidade Federal do Cear\'a, Fortaleza, Cear\'a, Brazil}
\author{S. Glancy}
\affiliation{Applied and Computational Mathematics Division, National Institute of Standards and Technology, Boulder, Colorado, 80305, USA}

%\collaboration{MUSO Collaboration}%\noaffiliation

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Quantum state tomography (QST) aims to determine the quantum state of a system from measured data and is an essential tool for quantum information. When dealing with quantum states of light, QST is done by measuring  quantum  noise  statistics  of the  field  amplitudes  at different  optical  phases using homodyne detection. The quadrature-phase homodyne measurement outputs a continuous variable, but we can histogram the continuous measurements and make the statistical estimation faster without losing too much information. This paper investigate different ways to determine the quadrature 
histograms for optical homodyne QST.

\end{abstract}

\pacs{
03.65.Wj, %State reconstruction, quantum tomography
03.67.-a, % Quantum information
42.50.Dv %Quantum state engineering and measurements in quantum optics
} % PACS, the Physics and Astronomy Classification Scheme.
%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents

\section{Introduction}
\label{intro}
Quantum information science and engineering is now at the point where 
rudimentary quantum computers are available in the laboratory and 
commercially~\cite{kandala2017,Linke2017,Monk2017,Denchev2016}.
Consequently, precise reconstruction and diagnostic tools used to estimate 
quantum states~\cite{Vogel1989, Smithey1993, Dunn1995, Banaszek1999, Banaszek2000, White2002, Ourjoumtsev2007, Neergaard2006}, 
processes~\cite{Chuang1997, Poyatos1997, Altepeter2003, Dariano1998, Nielsen1998, Mitchell2003, Obrien2004,Kupchak2015}, and 
measurements~\cite{Luis1999, Fiurasek2001, Dariano2004, Lundeen2009} are fundamental.

Quantum tomography techniques for quantum states of light became a subject of
major interest in recent years, since quantum light sources are essential
for implementations of continuous-variable (CV) quantum computation~\cite{Lloyd1999, Gottesman2001, Bartlett2002, Jeong2002, Ralph2003}. 
These source are also extensively exploited in quantum criptography~\cite{Ralph1999, Hillery2000, Silberhorn2002, Pirandola2008, Luiz2017}, 
quantum metrology~\cite{Eberle2010, Demkowicz2013}, state teleportation~\cite{Vaidman1994, Braunstein1998, He2015}, dense 
coding~\cite{Braunstein2000, Lee2014} and cloning~\cite{Cerf2000, Braunstein2001}. 

In quantum state tomography, we perform a large number of experimental measurements on
a collection of quantum systems, all prepared in a same unknown state. The goal is to
estimate this unknown state from the experimental measurements results. This estimation can be done from the experimental statistical data by different methods. In here we will be dealing with Maximum Likelihood Estimation (MLE), that finds among all possible states, the one which maximizes the probability of obtaining the experimental data set in hand. 

Quantum homodyne tomography is one of the most popular optical tomography techniques
available. It rapidly became a versatile tool and has been applied in many different quantum optics experimental settings since 
it was proposed by Vogel and Risken in 1989~\cite{Vogel1989} and first implemented by Smithey \textit{et al.} in 1993~\cite{Smithey1993}. 
This technique permits to characterize a light quantum state by means of the electric field quantum noise statistics 
collected through multiple phase-sensitive measurements. 

A homodyne measurement generates a continuous value. While no data binning is necessarily
needed, we believe that the loss due to binning may be insignificant. On the other hand,
discretization of the data by binning it reduces considerably the number of data, expediting the reconstruction algorithm. But how can we estimate the quadrature bin width, such that the bins are not too small nor too big? Bins should not be too small in order to guarantee a save in calculation time and memory. And bins should not be too large in order to avoid a lack of resolution, making the histogram a poor representation 
of the underlying distribution shape.
 
In this paper, we use numerical experiments to simulate optical homodyne tomography of quantum optical states and perform maximum likelihood tomography on the data with and without binning. When choosing a quadrature bin width, we use and compare two different ways: Scott's rule~\cite{Scott2010} and equation (5.136) from Leonhardt's book~\cite{Leonhardt1997}. The paper is divide as follow: in Section \ref{MLE} we review maximum likelihood in homodyne tomography.  In Section \ref{numerical-experiments} we describe our numerical experiments and present our results. In Section \ref{conclusion} we discuss the interpretation of our results and make some concluding remarks.


\section{Maximum likelihood in homodyne tomography}
\label{MLE}
Let us consider $N$ quantum systems, each of them prepared in an optical state described by a density matrix $\rhotrue$. In each experimental run, we measure the field quadrature at different phases $\theta$ of a local oscillator, i.e. a reference system prepared in a high amplitude coherent state. Each measurement is associated with an observable $\hat{X}_{\theta} = \hat{X} \cos \theta + \hat{P} \sin \theta$, where $\hat{X}$ and $\hat{P}$ are the position and momentum operators, respectively. For a given phase $\theta$, we measure a quadrature value $x$, resulting on a data set $\{(\theta_i, x_i)\}$.

The outcome of the $i$-th measurement is described by a positive-operator-valued measure (POVM) element $\Pi (x_i|\theta_i) = \Pi_i$. Given the data set \{$(\theta_{i},
x_i): i = 1, ..., N$\}, we can write the likelihood of a candidate density matrix $\rho$ as
\begin{eqnarray}
\mathcal{L} (\rho)= \prod_{i=1}^{N} \mathrm{Tr} (\Pi_i \rho),
\label{eq-likelihood}
\end{eqnarray}
where $\mathrm{Tr}(\rho \Pi_i)$ is the probability, when measuring with phase $\theta_i$, to obtain outcome $x_i$, according to the candidate density matrix $\rho$.

MLE searches within the density matrix space the one that maximizes the likelihood in~(\ref{eq-likelihood}). Equivalently, it may be convenient to maximize the logarithm of the likelihood (the ``log-likelihood''):
\begin{eqnarray}
L (\rho) = \ln \mathcal{L} (\rho)= \sum_{i=1}^{N} \ln [\mathrm{Tr} (\Pi_i \rho)],
\end{eqnarray} 
which is maximized by the same density matrix as the likelihood. The MLE is essentially a function optimization problem, and since the
log-likelihood function is concave, the convergence to an unique solution
will be achieved by most iterative optimization methods.

In our numerical simulations, we use an algorithm for likelihood maximization that begins with interactions of the $R\rho R$ algorithm~\cite{Rehacek2007} followed by iterations of a regularized gradient ascent algorithm (RGA). The main reason to switch from one algorithm to another is the fact that an expressive slow-down was observed in the $R\rho R$ algorithm after about $(n+1)^2/4$ iterations.



Our algorithm for likelihood maximization begins with several
iterations of the $R\rho R$ algorithm followed by iterations of a
regularized gradient ascent algorithm (RGA). After an initial period
of fast convergence, we have observed significant slow-down in the
$R\rho R$ algorithm after around $(n+1)^2/4$ iterations. To
alleviate this problem, after $(n+1)^{2}/4$ $R\rho R$ iterations, we
switch to the RGA.  Let $\rho^{(k)}$ be the density matrix found after
$k$ iterations, the first of which is provided by the last iteration
of $R \rho R$. In the RGA, $\rho^{(k+1)}$ is parametrized as
\begin{equation}
  \rho^{(k+1)}=\frac{\left(\sqrt{\rho^{(k)}}+A\right)\left(\sqrt{\rho^{(k)}}+A^{\dagger}\right)}{\mathrm{Tr}\left[\left(\sqrt{\rho^{(k)}}+A\right)\left(\sqrt{\rho^{(k)}}+A^{\dagger}\right)\right]},
\end{equation}
where $A$ may be any complex matrix of the same dimensions as $\rho$.
This construction ensures that $\rho^{(k+1)}$ is a physical density
matrix for any $A$.  To choose $A$, a quadratic approximation of the
log-likelihood is performed.  $A$ maximizes the quadratic
approximation of the log-likelihood subject to the constraint that
$\text{Tr}(AA^{\dagger})\leq u$, where $u$ is a positive number that
the algorithm adjusts to ensure that the log-likelihood increases with
each iteration.

All iterations halt when the stopping criterion of \cite{Glancy2012}
signals that the $L(\rho_{\text{ML}})-L(\rho^{(k)})\leq 0.2$, where
$L(\rho_{\text{ML}})$ is the maximum of the log-likelihood.  By
bounding the log-likelihood improvement that can be achieved with
further iterations, we ensure that the last iteration produces an
estimate that is ``close'' to $\rho_{\text{ML}}$, where that closeness
is statistically relevant \cite{Glancy2012}.

\section{Numerical experiments}
\label{numerical-experiments}
Our numerical experiments simulated single mode optical homodyne
measurements~\cite{Lvovsky2009} of a state created by sending a
squeezed vacuum state, with quadratures variances $s/2$ and $1/(2s)$,
through a lossy medium with transmissivity $t$.  These states are
Gaussian states with zero means, which can be parametrized by their
covariance matrices. Since we want to simulate states of different
purities, we will express purity as a function of squeezing and
transmissivity.

The covariance matrix of the state after the lossy medium is given by
\begin{eqnarray}
\Sigma = t \left(
\begin{array}{cc}
\frac{1}{2s} & 0 \\
0 & \frac{s}{2} 
\end{array} \right) + (1-t)
\frac{\mathbb{I}}{2}, 
\end{eqnarray}
where $\mathbb{I}$ is the identity matrix. Purity is then given by~\cite{Paris2003}
\begin{align}
p(s,t) & = \frac{1}{2\sqrt{\mathrm{Det}(\Sigma)}} \nonumber \\
& = \frac{1}{2\sqrt{\left( \frac{1}{2} - \frac{1}{4s} - \frac{s}{4}\right) \left(t^2 - t\right) + \frac{1}{4}}}. 
\label{eq-p(s,t)} 
\end{align}

Our numerical experiments begin with the choosing of a desired purity
for the true state. We use Eq. (\ref{eq-p(s,t)}) to obtain an $(s,t)$
pair that produces the desired purity.  The choice of $(s,t)$ is not
unique, so we use two strategies, described below, that give states
that are close to the vacuum and highly squeezed states.  We
represent the pure squeezed state with squeezing $s$ as
$\rho_{\text{pure}}$, a density matrix in the photon number basis,
truncated at $n$ photons.  We then simulate passage of the pure
squeezed state through a medium with transmissivity $t$ by a quantum
operation that is equivalent to appending an ancillary mode in the
vacuum state, acting on the two modes with the beam splitter
transformation, and tracing-out the ancillary mode.  This quantum
operation is expressed with the set of Kraus operators
$\{E_i(t)|i=1\ldots n\}$, and transforms $\rho_{\text{pure}}$ into
$\rhotrue = \sum_{i=1}^n E_i(t)\rho_{\text{pure}}E_i(t)^\dagger$.
This procedure gives us the density matrix of a state with the desired
purity, represented in an $n$ photon basis.

To compute the probability
$P(x|\theta) = \mathrm{Tr}(\rhotrue \Pi(x|\theta))$ to obtain homodyne
measurement result $x$ at phase $\theta$ from state $\rhotrue$, we
derive a representation of $\Pi(x|\theta)$ also in the $n$ photon
basis.  Let $|x\rangle$ be the $x$-quadrature eigenstate with
eigenvalue $x$ expressed in the photon number basis, and let
$U(\theta)$ be the phase evolution unitary operator.  For an ideal
homodyne measurement, we would compute the probability as
$\mathrm{Tr}[\rhotrue U(\theta)^\dagger|x\rangle\langle x|U(\theta)]$.
However, real homodyne detectors suffer from photon loss.  Because
this loss is part of the measurement device, we include it in the POVM
elements by expressing them as
$\Pi(x|\theta) = \sum_{i=1}^n E_i(\eta)^\dagger U(\theta)^\dagger
|x\rangle \langle x| U(\theta) E_i(\eta)$ \cite{Lvovsky2004}.  By
including the loss associated with the measurement device in the POVM
elements, we can estimate the state of the system before that loss
occurs.  For all of our numerical experiments, we use $\eta = 0.9$,
which is typical for state-of-the-art homodyne detectors.  To produce
random samples of homodyne measurement results, we use rejection
sampling~\cite{Kennedy1980} from the distribution given by
$P(x|\theta)$.

We use two different methods for choosing the phases at which the
homodyne measurements are performed.  In the first method, for each
quadrature measurement a random phase is chosen.  In the second method for a total of
$N$ measurements, measured at $m$ different phases, we divide the
upper-half-circle evenly among the $m$ phases between 0 and $\pi$ and
measure $N/m$ times at each phase. Measuring the
            quadrature only once for a very large number of phases is natural for
            experimental systems that slowly scan the phase while sampling quadratures. 
For other systems, it may be more convenient to fix the phase and
repeatedly measure the quadrature before changing the phase.  We
expect these two strategies to behave differently for three
reasons. (1) When measuring evenly spaced phases, we
obtain a histogram of quadrature measurements that allows us to
directly reconstruct the probability distribution of the quadrature at
each of the phases, but when measuring random phases, our
knowledge of the quadrature probability distribution at each phase is
quite poor, but we obtain samples at many more phases.  The statistics
of the two strategies are quite different, and one might expect
that estimates produce different
biases. (2)~\cite{Sugiyama2012} showed that bias is reduced if one
measures a qubit in the direction of its Bloch vector.  By increasing
the number of phases at which we measure the optical mode, we increase
the probability that we measure in a direction that points toward the
boundary of state space, which might reduce bias. (3) To obtain a
single maximum of the likelihood function, we require an
informationally complete set of measurement operators.  If the number
of phases is too small relative to the maximum number of photons $n$
in the Hilbert space, we do not have an informationally complete set
of measurement operators, and there will be a family of density
matrices, all of which maximize the likelihood. According to
  \cite{Leonhardt1997}, $n+1$ different phases are required to
  reconstruct a state that contains at most $n$ photons.  The
likelihood maximization algorithm identifies one of these density
matrices, and there may be systematic error introduced in the process.

To calculate the mean purity, we reconstruct each state 50 times, each
time obtaining the purity for the reconstructed state. We then
calculate the arithmetic mean of the 50 purities to obtain the ``mean
reconstructed purity''. Our estimate of the purity bias is the
difference between the mean reconstructed purity and the purity of the
true state. The uncertainty in each bias estimate (shown as error bars
in the figures) is the standard deviation of the mean of the
reconstructed purity. The relative sizes of the magnitude of the bias
(abs(bias)) and the standard deviation of the 50 purities
(std(purity)) are also of interest, so we make some statements about
them in the figure captions.

\subsection{Nearly vacuum state}



\section{Conclusion}
\label{conclusion}

We have used idealized numerical experiments to generate simulated
data under various conditions, performed tomography, and estimated
bias in the purity of the results. The mean reconstructed
purities of the highest purity states are significantly lower than the
corresponding true state purities. This result shows clear evidence of
bias in the tomography algorithm, even when the likelihood model is
correct.  In our simulations we did not see a strong relationship
  between purity bias and the number of phases used to measure the
  state, though it is possible that such a relationship for some
  combination of states and measurement conditions. The abs(bias)
appears to be slightly larger for tomography performed on highly
squeezed states than in nearly vacuum states.

In this work we have focused on the influence of the true state's
purity on bias of the estimated purity, finding that more pure true
states suffer from more bias toward lower purity states.  This agrees
with the results of \cite{Schwemmer2015}. However, whether this is a
general property for all states is an open question.  Our study has
focused on bias in estimates of squeezed thermal states, but other
states, especially non-Gaussian states, may have more or less bias.
More numerical experiments on a greater diversity of states and using
different measurement schemes would be informative as would
exploration of bias in other parameters.

In many of our numerical experiments, we find that the bias in purity
is significant compared to the standard deviation of the estimates of
purity.  This is particularly problematic if tools like the bootstrap
are used to assign uncertainties in quantum state tomography.  If a
non-parametric bootstrap is used, every bootstrapped estimate will be
similarly biased.  If a parametric bootstrap is used, the original
estimate is biased once and the bootstrapped estimates will be biased
a second time.  Bias correction methods exist for parametric
bootstrap, but they require the bias to be consistent for different
states \cite{Efron1993}.  This might be a reasonable approximation,
but we have seen that it is not strictly true. Because of the problems
caused by bias, it maybe helpful to use confidence intervals such as
those described in \cite{Blume-Kohout2012, Christandl2012, Faist2015}
to assign uncertainties to estimated parameters.  Unfortunately those
methods produce confidence regions that are significantly larger (and
more conservative) than those produced by bootstrap methods commonly
used for quantum state tomography.

\begin{acknowledgments}
We thank Kevin Coakley, Adam Keith, and Emanuel Knill for helpful
comments on the manuscript.  H. M. Vasconcelos thanks the Instituto
Nacional de Ci\^encia e Tecnologia de Informa\c c\~ao Qu\^antica
(INCT-IQ). G. B. Silva thanks Coordena\c c\~ao de Aperfei\c coamento
de Pessoal de N\'ivel Superior (CAPES) for financial support. This
work includes contributions of the National Institute of Standards and
Technology, which are not subject to U.S. copyright.
\end{acknowledgments}


% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
% Scott: My LaTeX does not know about spphys, but it is not necessary
% to specify a bibliography style.  revtex should authomatically use
% the correct style based on the documentclass.
\bibliography{histogram}   % name your BibTeX data base


% Non-BibTeX users please use%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.


%\end{thebibliography}


\end{document}

%
% ****** End of file apssamp.tex ******

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
